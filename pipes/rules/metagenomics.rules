import os.path
from os.path import join
import textwrap

samples = list(read_samples_file(config.get("samples_metagenomics")))

rule all_metagenomics:
    input:
        expand(join(config["data_dir"], config["subdirs"]["metagenomics"],
                    "{sample}.raw.{method}.krona.html"), sample=samples, method=['kraken', 'rna_bwa', 'rna_bwa_nodupes'])

rule all_metagenomics_contigs:
    input:
        expand(join(config["data_dir"], config["subdirs"]["metagenomics"],
                    "{sample}.rpsblast.tbl"), sample=samples),
        expand(join(config["data_dir"], config["subdirs"]["metagenomics"],
                    "{sample}.infernal.tbl"), sample=samples),
        expand(join(config["data_dir"], config["subdirs"]["metagenomics"],
                    "{sample}.blastn.report"), sample=samples),
        expand(join(config["data_dir"], config["subdirs"]["metagenomics"],
                    "{sample}.blastx.report"), sample=samples)


rule all_metagenomics_host_depleted:
    input:
        expand(os.path.join(config["data_dir"], config["subdirs"]["metagenomics"],
                    "{sample}.cleaned.{method}.report"), sample=samples, method=['kraken', 'rna_bwa']),
        expand(os.path.join(config["data_dir"], config["subdirs"]["metagenomics"],
                    "{sample}.cleaned.{method}.krona.html"), sample=samples, method=['kraken', 'rna_bwa'])


method_props = {
    'diamond': {
        'reads_ext': 'diamond.lca.gz',
    },
    'kraken': {
        'reads_ext': 'kraken.reads.gz',
    },
    'rna_bwa': {
        'reads_ext': 'rna_bwa.lca.gz',
    },
    'rna_bwa_nodupes': {
        'reads_ext': 'rna_bwa.lca_nodupes.gz',
    }
}

taxfiles = [
                'names.dmp',
                'nodes.dmp',
                'merged.dmp'
            ]

taxfiles_kraken = [
                'names.dmp',
                'nodes.dmp'
]

taxfiles_krona = [
                'taxonomy.tab',
]

rna_bwa_ext = ['sa', 'bwt', 'amb', 'sa', 'ann', 'pac']


rule diamond:
    input:
        bam         = os.path.join(config["data_dir"], config["subdirs"]["per_sample"], "{sample}.{adjective}.bam"),
        diamond_db  = objectify_remote(expand("{path_prefix}.{ext}", path_prefix=config["diamond_db"], ext=["dmnd"])),
        taxonomy_db = objectify_remote(expand("{path_prefix}/{taxfile}", path_prefix=config["taxonomy_db"], taxfile=taxfiles))
    output:
        report = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.diamond.report"),
        reads    = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.diamond.lca.gz")
    resources:
        threads = int(config.get("number_of_threads", 1)),
        mem     = 120
    params:
        UGER = config.get('UGER_queues', {}).get('long', '-l h_rt=36:00:00')
    run:
        diamond_db_prefix  = os.path.splitext(strip_protocol(config["diamond_db"], relative=True))[0]
        taxonomy_db_prefix = strip_protocol(config["taxonomy_db"], relative=True)
        shell("{config[bin_dir]}/metagenomics.py diamond {input.bam} "+diamond_db_prefix+" "+taxonomy_db_prefix+" {output.report} --outReads {output.reads} --threads {resources.threads}")

rule align_rna:
    input:
        bam         = os.path.join(config["data_dir"], config["subdirs"]["per_sample"], "{sample}.{adjective}.bam"),
        rna_bwa_db  = objectify_remote(expand("{path_prefix}.{ext}", path_prefix=config["align_rna_db"], ext=rna_bwa_ext)),
        taxonomy_db = objectify_remote(expand("{path_prefix}/{taxfile}", path_prefix=config["taxonomy_db"], taxfile=taxfiles))
    output:
        report         = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.rna_bwa.report"),
        nodupes_report = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.rna_bwa.nodupes.report"),
        bam            = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.rna_bwa.bam"),
        lca            = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.rna_bwa.lca.gz"),
        nodupes_lca    = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.rna_bwa.lca_nodupes.gz")
    resources:
        threads = int(config.get("number_of_threads", 1)),
        mem     = 7
    params:
        UGER = config.get('UGER_queues', {}).get('long', '-l h_rt=36:00:00')
    run:
        rna_bwa_path_prefix = strip_protocol(config["align_rna_db"], relative=True)
        taxonomy_db_prefix = strip_protocol(config["taxonomy_db"], relative=True)
        shell("{config[bin_dir]}/metagenomics.py align_rna {input.bam} "+rna_bwa_path_prefix+" "+taxonomy_db_prefix+" {output.nodupes_report} --dupeReport {output.report} --outBam {output.bam} --outReads {output.nodupes_lca} --dupeReads {output.lca} --JVMmemory {resources.mem}g --threads {resources.threads}")

if config['kraken_execution'] == 'multiple':
    kraken_samples = []
    all_kraken_inputs = {}
    all_kraken_reports = {}
    all_kraken_reads = {}
    for adjective in ('raw', 'cleaned'):
        for sample in samples:
            kraken_report = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective}.kraken.report".format(
                sample=sample, adjective=adjective))
            kraken_input = os.path.join(config["data_dir"], config["subdirs"]["per_sample"], "{sample}.{adjective}.bam".format(
                sample=sample, adjective=adjective))
            try:
                itime = os.path.getmtime(kraken_input)
                otime = os.path.getmtime(kraken_report)
            except OSError:
                kraken_samples.append(sample)
                continue
            if itime > otime:
                kraken_samples.append(sample)
        if not kraken_samples:
            kraken_samples = samples.copy()

        all_kraken_inputs[adjective] = expand(
            os.path.join(config["data_dir"], config["subdirs"]["per_sample"], "{sample}.{adjective}.bam"),
            sample=kraken_samples, adjective=adjective)
        all_kraken_reports[adjective] = expand(
            os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective}.kraken.report"),
            sample=kraken_samples, adjective=adjective)
        all_kraken_reads[adjective] = expand(
            os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective}.kraken.reads.gz"),
            sample=kraken_samples, adjective=adjective)

    kraken_shell = textwrap.dedent("""\
    {config[bin_dir]}/metagenomics.py kraken {params.kraken_db} {input.bams} --outReads {output.reads} --outReports {output.reports} \
    {params.lock_memory} --threads {resources.threads}
    """)

    rule kraken_multiple_raw:
        input:
            bams = all_kraken_inputs['raw']
        output:
            reports = all_kraken_reports['raw'],
            reads = all_kraken_reads['raw']
        resources:
            threads = int(config.get("number_of_threads", 1)),
            mem     = 120
        params:
            UGER = config.get('UGER_queues', {}).get('long', '-l h_rt=36:00:00'),
            kraken_db = config['kraken_db'],
            lock_memory = ' --lockMemory' if config.get('kraken_lock_memory') else ''
        shell:
            kraken_shell

    rule kraken_multiple_cleaned:
        input:
            bams = all_kraken_inputs['cleaned']
        output:
            reports = all_kraken_reports['cleaned'],
            reads = all_kraken_reads['cleaned']
        resources:
            threads = int(config.get("number_of_threads", 1)),
            mem     = 120
        params:
            UGER = config.get('UGER_queues', {}).get('long', '-l h_rt=36:00:00'),
            kraken_db = config['kraken_db'],
            lock_memory = ' --lockMemory' if config.get('kraken_lock_memory') else ''
        shell:
            kraken_shell

else:
    rule kraken:
        input:
            bam = os.path.join(config["data_dir"], config["subdirs"]["per_sample"], "{sample}.{adjective}.bam")
        output:
            report = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.kraken.report"),
            reads  = os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.kraken.reads.gz")
        resources:
            threads = int(config.get("number_of_threads", 1)),
            mem     = 120
        params:
            UGER = config.get('UGER_queues', {}).get('long', '-l h_rt=36:00:00'),
            kraken_db = config['kraken_db'],
            lock_memory = ' --lockMemory' if config.get('kraken_lock_memory') else ''
        shell:
            """
            {config[bin_dir]}/metagenomics.py kraken {params.kraken_db} {input.bam} --outReads {output.reads} --outReports {output.report} \
            {params.lock_memory} --threads {resources.threads}
            """


rule all_kraken:
    input:
        expand(os.path.join(config["data_dir"], config["subdirs"]["metagenomics"],
                            "{sample}.raw.kraken.report"), sample=samples),
        expand(os.path.join(config["data_dir"], config["subdirs"]["metagenomics"],
                            "{sample}.raw.kraken.krona.html"), sample=samples)

rule all_kraken_host_depleted:
    input:
        expand(os.path.join(config["data_dir"], config["subdirs"]["metagenomics"],
                            "{sample}.cleaned.kraken.report"), sample=samples),
        expand(os.path.join(config["data_dir"], config["subdirs"]["metagenomics"],
                            "{sample}.cleaned.kraken.krona.html"), sample=samples)

rule krona_import_taxonomy:
    input:
        tsv = lambda wildcards: os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], \
           ".".join([wildcards.sample, wildcards.adjective, method_props[wildcards.method]['reads_ext']])),
        krona_db = objectify_remote(expand("{path_prefix}/{kronafile}", path_prefix=config["krona_db"], kronafile=taxfiles_krona))#["taxonomy.tab", "gi_taxid.dat"]))
    output:
        os.path.join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.{adjective,raw|cleaned}.{method,kraken|diamond|rna_bwa|rna_bwa_nodupes}.krona.html")
    resources:
        mem=32
    run:
        krona_db_prefix = strip_protocol(config["krona_db"], relative=True)
        shell("{config[bin_dir]}/metagenomics.py krona {input.tsv} {krona_db_prefix} {output} --norank")

rule unclassified_trinity:
    ''' This step runs the Trinity assembler on Kraken unclassified reads.
        First trim reads with trimmomatic, rmdup with prinseq,
        and random subsample to no more than 100k reads.
    '''
    # input: join(config["data_dir"], config["subdirs"]["per_sample"], "{sample}.raw.bam")
    input: bam=join(config["data_dir"], config["subdirs"]["per_sample"], "{sample}.raw.bam"),
           kraken=join(config["data_dir"], config["subdirs"]["metagenomics"], "{sample}.raw.kraken.reads.gz")
    output: join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.assembly-unclassified-trinity.fasta')
    resources:
            mem=16,
            cores=int(config.get("number_of_threads", 1))
    params: extracted=join(config["tmp_dir"], config["subdirs"]["metagenomics"], "{sample}.kraken-unclassified.bam"),
            logid="{sample}",
            clipDb=config["trim_clip_db"],
            subsamp_bam=config["tmp_dir"]+'/'+config["subdirs"]["assembly"]+'/{sample}.subsamp.bam',
            numThreads=str(config.get("number_of_threads", 1))
    shell:
        '''
        {config[bin_dir]}/metagenomics.py kraken_unclassified {input.kraken} {input.bam} {params.extracted}
        {config[bin_dir]}/assembly.py assemble_trinity {params.extracted} {params.clipDb} {output} --n_reads 100000 --outReads {params.subsamp_bam} --threads {params.numThreads} --JVMmemory 14g
        '''

rule cluster_contigs:
    input: join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.assembly-unclassified-trinity.fasta')
    output: join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.clustered.fasta')
    resources:
        mem=4,
        cores=int(config.get("number_of_threads", 1))
    shell:
        """
        {config[bin_dir]}/metagenomics.py cluster_contigs {input} {output} --memory {resources.mem} --memory {resources.mem} --numThreads {resources.cores}
        """

rule blast_taxonomy:
    input: join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.clustered.fasta')
    output: blastn=join(config["data_dir"], config["subdirs"]["metagenomics"], '{sample}.blastn.m8'),
            blastn_report=join(config["data_dir"], config["subdirs"]["metagenomics"], '{sample}.blastn.report')
    resources:
        mem=24,
        cores=int(config.get("number_of_threads", 1))
    shell:
        """
        {config[bin_dir]}/metagenomics.py blast_taxonomy {input}  \
          --ntDb {config[blast_nt]} --outBlastn {output.blastn} --outBlastnReport {output.blastn_report}  \
          --taxDb {config[taxonomy_db]} --numThreads {resources.cores}
        """

rule blastx_taxonomy:
    input: join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.clustered.fasta')
    output: blastx=join(config["data_dir"], config["subdirs"]["metagenomics"], '{sample}.blastx.m8'),
            blastx_report=join(config["data_dir"], config["subdirs"]["metagenomics"], '{sample}.blastx.report')
    resources:
        mem=24,
        cores=int(config.get("number_of_threads", 1))
    shell:
        """
        {config[bin_dir]}/metagenomics.py blast_taxonomy {input}  \
          --nrDb {config[blast_nr]} --outBlastx {output.blastx} --outBlastxReport {output.blastx_report}  \
          --taxDb {config[taxonomy_db]} --numThreads {resources.cores}
        """

rule rpsblast_contigs:
    input: join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.clustered.fasta')
    output: join(config["data_dir"], config["subdirs"]["metagenomics"], '{sample}.rpsblast.tbl')
    resources:
        mem=32,
        cores=int(config.get("number_of_threads", 1))
    params: ORFS=join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.orfs.fna')
    shell:
        """
        {config[bin_dir]}/metagenomics.py rpsblast_models {config[rpsblast_dbs]} {input} {output} --numThreads {resources.cores} --orfs {params.ORFS}
        """

rule infernal_contigs:
    input: join(config["tmp_dir"], config["subdirs"]["metagenomics"], '{sample}.clustered.fasta')
    output: join(config["data_dir"], config["subdirs"]["metagenomics"], '{sample}.infernal.tbl')
    resources:
        mem=16,
        cores=int(config.get("number_of_threads", 1))
    shell:
        """
        {config[bin_dir]}/metagenomics.py infernal_contigs {config[infernal_db]} {input} {output} --numThreads {resources.cores}
        """
